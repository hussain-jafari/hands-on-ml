{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 Up and Running with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "# Explicitly calculate best parameter values\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different ways to calculate gradients.\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "Here, we will do so explicitly using the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data for gradient descent\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE:  2.7544262\n",
      "Epoch 100 MSE:  0.632222\n",
      "Epoch 200 MSE:  0.5727805\n",
      "Epoch 300 MSE:  0.5585007\n",
      "Epoch 400 MSE:  0.54907\n",
      "Epoch 500 MSE:  0.542288\n",
      "Epoch 600 MSE:  0.53737885\n",
      "Epoch 700 MSE:  0.533822\n",
      "Epoch 800 MSE:  0.5312425\n",
      "Epoch 900 MSE:  0.5293705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855226e+00],\n",
       "       [ 7.74078071e-01],\n",
       "       [ 1.31192386e-01],\n",
       "       [-1.17845066e-01],\n",
       "       [ 1.64778143e-01],\n",
       "       [ 7.44081801e-04],\n",
       "       [-3.91945131e-02],\n",
       "       [-8.61356556e-01],\n",
       "       [-8.23479712e-01]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in np.arange(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE: ', mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff\n",
    "\n",
    "Or we can have TensorFlow calculate it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE:  2.7544262\n",
      "Epoch 100 MSE:  0.632222\n",
      "Epoch 200 MSE:  0.5727805\n",
      "Epoch 300 MSE:  0.5585007\n",
      "Epoch 400 MSE:  0.54907\n",
      "Epoch 500 MSE:  0.54228795\n",
      "Epoch 600 MSE:  0.5373789\n",
      "Epoch 700 MSE:  0.533822\n",
      "Epoch 800 MSE:  0.5312425\n",
      "Epoch 900 MSE:  0.5293704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249e+00],\n",
       "       [ 7.74078071e-01],\n",
       "       [ 1.31192386e-01],\n",
       "       [-1.17845066e-01],\n",
       "       [ 1.64778143e-01],\n",
       "       [ 7.44078017e-04],\n",
       "       [-3.91945094e-02],\n",
       "       [-8.61356676e-01],\n",
       "       [-8.23479772e-01]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "# Only different line\n",
    "# TensorFlow calculates gradient for you\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in np.arange(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE: ', mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE:  2.7544262\n",
      "Epoch 100 MSE:  0.632222\n",
      "Epoch 200 MSE:  0.5727805\n",
      "Epoch 300 MSE:  0.5585007\n",
      "Epoch 400 MSE:  0.54907\n",
      "Epoch 500 MSE:  0.54228795\n",
      "Epoch 600 MSE:  0.5373789\n",
      "Epoch 700 MSE:  0.533822\n",
      "Epoch 800 MSE:  0.5312425\n",
      "Epoch 900 MSE:  0.5293704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249e+00],\n",
       "       [ 7.74078071e-01],\n",
       "       [ 1.31192386e-01],\n",
       "       [-1.17845066e-01],\n",
       "       [ 1.64778143e-01],\n",
       "       [ 7.44078017e-04],\n",
       "       [-3.91945094e-02],\n",
       "       [-8.61356676e-01],\n",
       "       [-8.23479772e-01]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# Two different lines\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE: \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "If we'd like to change the values of the data we feed into our algorithm (like we do with mini-batches), we can use placeholder nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0547962 ],\n",
       "       [ 0.81847644],\n",
       "       [ 0.12269205],\n",
       "       [-0.22543088],\n",
       "       [ 0.2628372 ],\n",
       "       [ 0.00897413],\n",
       "       [-0.05440009],\n",
       "       [-0.89921325],\n",
       "       [-0.8637755 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def create_batch(batch_size):\n",
    "    random_indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[random_indices]\n",
    "    y_batch = housing.target.reshape(-1,1)[random_indices]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.arange(n_batches):\n",
    "            X_batch, y_batch = create_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models\n",
    "\n",
    "To save and restore models, we can create a Saver node after creating all our variable nodes, and calling its save method whenever we want to store the current contents of our model. Restoring them is as simple as calling the node's restore method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0, seed=42), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create Saver node\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in np.arange(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            saver.save(sess, '/tmp/temp_model.ckpt') # Every 100 epochs\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    saver.save(sess, '/tmp/final_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 hussain hussain     36 Mar 10 13:50 /tmp/final_model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 hussain hussain    129 Mar 10 13:50 /tmp/final_model.ckpt.index\r\n",
      "-rw-r--r-- 1 hussain hussain 832622 Mar 10 13:50 /tmp/final_model.ckpt.meta\r\n",
      "-rw-r--r-- 1 hussain hussain     36 Mar 10 13:50 /tmp/temp_model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 hussain hussain    129 Mar 10 13:50 /tmp/temp_model.ckpt.index\r\n",
      "-rw-r--r-- 1 hussain hussain 832622 Mar 10 13:50 /tmp/temp_model.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al /tmp/*model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/20190310135013\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "logdir_root = 'tf_logs'\n",
    "logdir = f'{logdir_root}/{current_time}'\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in np.arange(n_epochs):\n",
    "        for batch_number in np.arange(n_batches):\n",
    "            X_batch, y_batch = create_batch(batch_size)\n",
    "            if batch_number % 10 == 0: # every 10th batch\n",
    "                summary_str = mse_summary.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "                step = epoch * batch_size + batch_number\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Scopes\n",
    "\n",
    "Scopes will group nodes into one larger collapsible namespace on TensorBoard. For example, to collapse error and mse into one namespace called 'loss', we could write the following where they're defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity\n",
    "\n",
    "We can combine name scopes with the idea of modularizing repeated tasks to write better code. Let's create five ReLUs and sum their output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Rectilinear activation function\n",
    "def relu(X):\n",
    "    with tf.name_scope('relu'):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "        b = tf.Variable(0.0, name='bias')\n",
    "        z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "        return tf.maximum(0., z, name='max')\n",
    "    \n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = [relu(X) for _ in np.arange(5)]\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables\n",
    "\n",
    "If we'd like to avoid sharing variables by passing them as parameters every time we need them, we can also use the get_variable() function to share variables between different parts of our graph, passing reuse=True or calling scope.reuse_variablees() to use a variable that has already been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(x):\n",
    "    threshold = tf.get_variable('threshold', shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "    return tf.maximum(threshold, z, name='max')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
